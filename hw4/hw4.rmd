---
title: "PA 446 Homework 4"
author: "Alexis Kwan, kwan1, UID: 655727984"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

```{r echo=FALSE, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE)
```

```{r echo=FALSE, eval=FALSE}
# 2020
download.file(url = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/6ff6a6fd-3141-4440-a880-6f60a37fe789/download/script_105774672_20210108153400_combine.csv", destfile = "311_service_requests_2020.csv", mode = "w")

# GET('https://data.boston.gov/api/3/action/datastore_search?resource_id=6ff6a6fd-3141-4440-a880-6f60a37fe789', write_disk("311_service_requests_2020.csv", overwrite=TRUE))

# 2021
download.file(url = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/f53ebccd-bc61-49f9-83db-625f209c95f5/download/tmpywju6ai2.csv", destfile = "311_service_requests_2021.csv", mode = "w")

# GET('https://data.boston.gov/api/3/action/datastore_search?resource_id=f53ebccd-bc61-49f9-83db-625f209c95f5', write_disk("311_service_requests_2021.csv", overwrite=TRUE))
```


```{r echo=FALSE}
library(tidyverse)
library(lubridate)
sr2020 = read_csv("311_service_requests_2020.csv")
sr2021 = read_csv("311_service_requests_2021.csv")
sr_all = bind_rows(sr2020, sr2021) %>%
   filter(!is.na(closed_dt) & !is.na(open_dt)) %>%
   mutate(close_time = as.duration(closed_dt - open_dt))
```

```{r include=FALSE}
head(sr_all, 5)
tail(sr_all, 5)
table(sr_all$ontime)
sr_all %>% mutate(days = close_time/ddays(1)) %>%
   select(close_time, days)
```

1. How long does it take to complete a request city-wide (hence known as close time)? Please provide 3 metrics to answer this question.
   - Why did you choose the 3 metrics?
   - What does the 3 metrics say collectively? For example, if one of your metrics is the mean, what might that number miss? What other metrics might you want to pull to supplement what the mean misses.
   
Suppose for a moment that we could believe that a mean statistic would represent the close time well. When we calculate that we get:
```{r}
sr_all %>%
   summarise(close_time_mean = seconds_to_period(mean(close_time)))
```

Six days sounds like a long time. However would that make sense if the data looks as it does below?

```{r}
sr_all %>%
   ggplot(aes(close_time/ddays(1))) +
   geom_histogram(binwidth = 1) +
   xlim(0,20) +
   ylim(0,75000) +
   labs(title = "Histogram of Time Between Open and Close Date of Request",
        y = "Count",
        x = "Close Time in Days")
```

We see that the distribution for the time between when a request is opened and closed is very skewed. So if we were to measure or represent how long it would take to complete a request, we should at least use a statistic that is robust to outliers and skew. If we take a median we get a median close time of about 8.5 hours.

```{r}
sr_all %>%
   select(open_dt, closed_dt, close_time) %>%
   mutate(close_time_median = median(as.period(close_time))) %>%
   filter(row_number(close_time_median) == 1) %>%
   select(close_time_median)
```

However, if we break the close time down by the department for which the request is for, we see the distributions aren't so simple. In fact it appears that for some departments the distribution may be multi-modal, and there appears to be clear differences between departments. This likely reflects the difference in nature of the requests submitted to each department. 

```{r}
library(ggridges)

sr_all %>%
   ggplot(aes((close_time/ddays(1)), y = department, fill = department)) +
   geom_density_ridges(alpha=0.6) +
   theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
   ) +
   xlim(0,60) +
   labs(y = "Department",
        x = "Close Time in Days")
```

While median is robust to outliers, it may not capture enough information on the width of the distribution. So what if used multiple statistics instead of just one?

```{r}
sr_all %>%
   mutate(close_time = as.period(closed_dt - open_dt)) %>%
   summarise(quantile = scales::percent(c(0.5,0.75,0.9)),
             close_time_hours = quantile(hour(close_time), c(0.5,0.75,0.9)))
```

Based on these quantiles we can say that there is a 75% probability that your request will be finished in 14 hours and 90% that it will be completed in 20 hours. So most likely it will be finished in a day at most, across all departments. This seems to capture the width of the distributions better.

2. Assume that the primary goal of your analysis is to ensure that all city departments have as short of a close time as possible. 
   - What are the 3 departments that the City should focus on? Note the language here. Yes it is vague. It will be up to you to define what “should focus on” is and then conduct your analysis.
   - What are the 3 departments that have done well so far?
   
```{r}
sr_all %>%
   mutate(close_time = closed_dt - open_dt) %>%
   group_by(department) %>%
   summarize(close_time_median = as.period(median(close_time))) %>%
   arrange(desc(close_time_median))
```

There are several ways to minimize close time. We could minimize the largest contributors, just based on department median close time, to the overall close time across departments. From the list of median close times shown above it would appear then that the departments that the City should focus on should be Property Management, Inspectional Services, and the Water and Sewer Commission. By the same measure, the best departments are the Transportation Department, Public Works and Animal Control, which I assume is ANML since it's not defined in the data dictionary. I ignore both DND_ and GEN_ since they are not defined in the data dictionary.

```{r}
sr_all %>%
   count(department) %>%
   arrange(desc(n))
```

However, given the most requests are for PWDx, BTDT, and ISD, it might make more sense to focus on those departments instead, since they make up the largest proportion of the overall close time.

3. In words, describe 2 confounding factors that can impact the close time of a ticket. You must be able to calculate these confounding factors from the dataset.

```{r include=FALSE}
lm0 = lm(close_time/ddays(1) ~ department + reason + neighborhood, data = sr_all)
summary(lm0)
```

If we suppose that the department is the main reason for the close time of the requests, we can easily see how the reason for the request would be a confounding factor. Departments are usually centered around certain objectives and the reasons will be directly related to those reasons. Neighborhood could also be a confounding factor for more subtle reasons. Neighborhoods that have been divested from are far from service or administration centers may require more time for requests. 

4. In words, describe a confounding factor that can impact the close time of a ticket. This factor will come from an outside dataset.
   - Describe how you might be able to find this dataset.
   - Describe exactly how you will join this outside dataset to the 311 ticket data
   
In terms what I was alluding to earlier with factoring neighborhood into close time, we can understand it as a proxy for population characteristics like race and income (because of historic divestment, segregation, etc.). Based on this assumption we can then use Census data, specifically income and race data from the American Community Survey (table S1903). With this information we would essentially be exploring the question, "does close time vary by the makeup of an area of the city?" To join this dataset with our current one, there are several paths but the simplest way would be to join via zip code, which will then give us median incomes for each race for every zip code in our dataset.